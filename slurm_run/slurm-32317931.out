[2020-07-07 09:48:30,790 INFO] Loading data from /global/cfs/cdirs/atlas/achkodrov/pixel-NN-master/training1.h5
[2020-07-07 09:48:56,777 INFO] Building model from /global/cfs/cdirs/atlas/achkodrov/pixel-NN-master/share/reference_number.py
2020-07-07 09:49:02.139881: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2020-07-07 09:49:02.170878: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2300085000 Hz
2020-07-07 09:49:02.173976: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55555df74e70 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-07-07 09:49:02.173989: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-07-07 09:49:02.174109: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.
[2020-07-07 09:49:02,386 INFO] Compiling model
[2020-07-07 09:49:02,392 INFO] Fitting model
Epoch 1/1000

Epoch 00001: val_loss improved from inf to 0.40546, saving model to /global/cfs/cdirs/atlas/achkodrov/pixel-NN-master/output/TrainedNN_7_6.h5
176225/176225 - 568s - loss: 0.5806 - val_loss: 0.4055
Epoch 2/1000

Epoch 00002: val_loss improved from 0.40546 to 0.34201, saving model to /global/cfs/cdirs/atlas/achkodrov/pixel-NN-master/output/TrainedNN_7_6.h5
176225/176225 - 580s - loss: 0.5210 - val_loss: 0.3420
Epoch 3/1000

Epoch 00003: val_loss did not improve from 0.34201
176225/176225 - 587s - loss: 0.5027 - val_loss: 0.3719
Epoch 4/1000

Epoch 00004: val_loss did not improve from 0.34201
176225/176225 - 578s - loss: 0.4953 - val_loss: 0.3528
Epoch 5/1000

Epoch 00005: val_loss did not improve from 0.34201
176225/176225 - 580s - loss: 0.4912 - val_loss: 0.3516
Epoch 6/1000

Epoch 00006: val_loss did not improve from 0.34201
176225/176225 - 602s - loss: 0.4888 - val_loss: 0.3753
Epoch 7/1000

Epoch 00007: val_loss did not improve from 0.34201
176225/176225 - 580s - loss: 0.4868 - val_loss: 0.4019
Epoch 8/1000

Epoch 00008: val_loss did not improve from 0.34201
176225/176225 - 574s - loss: 0.4853 - val_loss: 0.4583
Epoch 9/1000

Epoch 00009: val_loss did not improve from 0.34201
176225/176225 - 577s - loss: 0.4839 - val_loss: 0.4171
Epoch 10/1000

Epoch 00010: val_loss did not improve from 0.34201
176225/176225 - 584s - loss: 0.4828 - val_loss: 0.3877
Epoch 11/1000

Epoch 00011: val_loss did not improve from 0.34201
176225/176225 - 576s - loss: 0.4818 - val_loss: 0.3855
Epoch 12/1000

Epoch 00012: val_loss did not improve from 0.34201
176225/176225 - 581s - loss: 0.4809 - val_loss: 0.3425
Epoch 13/1000

Epoch 00013: val_loss did not improve from 0.34201
176225/176225 - 583s - loss: 0.4801 - val_loss: 0.4123
Epoch 14/1000

Epoch 00014: val_loss did not improve from 0.34201
176225/176225 - 573s - loss: 0.4792 - val_loss: 0.4004
Epoch 15/1000

Epoch 00015: val_loss did not improve from 0.34201
176225/176225 - 581s - loss: 0.4785 - val_loss: 0.4246
Epoch 16/1000

Epoch 00016: val_loss did not improve from 0.34201
176225/176225 - 580s - loss: 0.4779 - val_loss: 0.4014
Epoch 17/1000

Epoch 00017: val_loss did not improve from 0.34201
176225/176225 - 566s - loss: 0.4773 - val_loss: 0.4229
Epoch 18/1000

Epoch 00018: val_loss did not improve from 0.34201
176225/176225 - 579s - loss: 0.4769 - val_loss: 0.4962
Epoch 19/1000

Epoch 00019: val_loss did not improve from 0.34201
176225/176225 - 576s - loss: 0.4764 - val_loss: 0.3791
Epoch 20/1000

Epoch 00020: val_loss did not improve from 0.34201
176225/176225 - 582s - loss: 0.4758 - val_loss: 0.4265
Epoch 21/1000

Epoch 00021: val_loss did not improve from 0.34201
176225/176225 - 582s - loss: 0.4754 - val_loss: 0.4459
Epoch 22/1000

Epoch 00022: val_loss did not improve from 0.34201
176225/176225 - 581s - loss: 0.4749 - val_loss: 0.4118
Epoch 23/1000

Epoch 00023: val_loss did not improve from 0.34201
176225/176225 - 578s - loss: 0.4745 - val_loss: 0.4253
Epoch 24/1000

Epoch 00024: val_loss did not improve from 0.34201
176225/176225 - 580s - loss: 0.4742 - val_loss: 0.4292
Epoch 25/1000

Epoch 00025: val_loss did not improve from 0.34201
176225/176225 - 585s - loss: 0.4739 - val_loss: 0.3997
Epoch 26/1000

Epoch 00026: val_loss did not improve from 0.34201
176225/176225 - 576s - loss: 0.4736 - val_loss: 0.3976
Epoch 27/1000

Epoch 00027: val_loss did not improve from 0.34201
176225/176225 - 600s - loss: 0.4734 - val_loss: 0.4374
Epoch 28/1000

Epoch 00028: val_loss did not improve from 0.34201
176225/176225 - 580s - loss: 0.4732 - val_loss: 0.4437
Epoch 29/1000

Epoch 00029: val_loss did not improve from 0.34201
176225/176225 - 588s - loss: 0.4729 - val_loss: 0.4308
Epoch 30/1000

Epoch 00030: val_loss did not improve from 0.34201
176225/176225 - 578s - loss: 0.4728 - val_loss: 0.3489
slurmstepd: error: *** STEP 32317931.0 ON nid00945 CANCELLED AT 2020-07-07T14:48:01 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 32317931 ON nid00945 CANCELLED AT 2020-07-07T14:48:01 DUE TO TIME LIMIT ***
srun: got SIGCONT
srun: forcing job termination
